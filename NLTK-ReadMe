pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl

To update PIP
                python -m pip install -U pip

To install NLTK
                sudo pip install -U nltk

To install Numpy
                sudo pip install -U numpy

To download all the corpus of NLTK data
                import nltk
                nltk.download('all')

http://pyvideo.org/pydata-london-2017/building-a-chatbot-with-python-nltk-and-scikit.html

Python ---------------   NLTK ---------------------

Tokenization:-  is a method of breaking up a piece of text into many pieces, such as sentences and words,
WordNet:- is a dictionary designed for programmatic access by natural language processing systems
                following usefulness of WordNet:-
                                a. Looking up the definition of a word
                                b. Finding synonyms and antonyms
                                c. Exploring word relations and similarity
                                d. Word sense disambiguation for words that have multiple uses and definitions

A 'corpus' is just a body of text and 'corpus readers' are designed to make accessing a corpus much easier than direct file access.             

@ Start with NLTK example

			from nltk.tokenize import sent_tokenize
			import nltk.data

			para = "Hello World. It's good to see you. Thanks for buying this book."
			sentense_token = sent_tokenize(para)
			print(sentense_token)
				  or
			import nltk.data
			tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')
			tokenizer.tokenize(para)

			Both do the same thing but the only difference is ""sent_tokenize" load the pickle data on demand. So if you're going to be tokenizing a lot of sentences,
			it's more efficient to load the PunktSentenceTokenizer class once, and call its tokenize() method.

nltk.tokenize.sent_tokenize -> is a sentences tokenizer. It divide paragraph into sentence.
nltk.tokenize.word_tokenize -> is a word tokenizer. It will create a work token from sentence.
     word_tokenize is a wapper over the TreebankWordTokenizer. Internally it will call the tokenize method of TreebankWordTokenizer.

All the tokenizer inherit the TokenizerI interface

                TokenizerI
                    |
                    |
                     --> PunktWordTokenizer
                     --> TreebankWordTokenizer
                               |
                               |
                                --> RegexpTokenizer
                                        |
                                        |
                                        --> WordPunctTokenizer
                                        --> WhitespaceTokenizer
        One of the tokenizer's most significant conventions is to separate contractions

@ use of TreebankWordTokenizer

                like :- >>> from nltk.tokenize import word_tokenize
                        >>> word_tokenize('Can't wait')
                output  >>> ['ca', "n't", 'wait']

				The output of work_tokenizer/TreebankWordTokenizer is not acceptable so we choice tokenizer extending regular expressions.

@ user of PunktWordTokenizer
                        >>> from nltk.tokenize import PunktWordTokenizer
                        >>> tokenizer = PunktWordTokenizer()
                        >>> tokenizer.tokenize("Can't is a contraction.")
                output  >>> ['Can', "'t", 'is', 'a', 'contraction.']

	          It splits on punctuation, but keeps it with the word instead of creating separate tokens.

@ use of WordPunctTokenizer
                        >>> from nltk.tokenize import WordPunctTokenizer
                        >>> tokenizer = WordPunctTokenizer()
                        >>> tokenizer.tokenize("Can't is a contraction.")
                 output >>> ['Can', "'", 't', 'is', 'a', 'contraction', '.']

                    It splits all punctuation into separate tokens:

@ use of RegexpTokenizer
        by using RegexpTokenizer you can create your own tokenizer. but use it carefully.

@Filtering stopwords in a tokenized

                1 >>> from nltk.corpus import stopwords
                2 >>> from string import punctuation
                3 >>> from nltk.tokenize import WordPunctTokenizer
                4 >>> english_stopword = set(stopwords.words('english')+list(punctuation))
                5 >>> sentence ='You can\'t do that! This is a passenger train...The blood of Jesus Christ! You can\'t do that; this is a passenger train! You need to find Jesus!...That is the devil\'s drink. By the blood of Jesus you need to repent!'
                6 >>> tokenizer= WordPunctTokenizer()
                7 >>> wordList = tokenizer.tokenize(sentence)
                8 >>> withoutStopword = [word for word in wordList if word not in english_stopword]
                9 >>> print(withoutStopword)

                @ line no 1 we are importing stopword those are declared n /nltk_data/corpora/stopwords, even we can add more stopwords
                @ line no 2 we are importing all the punctuation define string like :- !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
                @ line no 4 we create a set of all stopword and punctuation
                @ line no 6 we create WordPunctTokenizer that convert the sentence into word phase that done in line no 7
                @ line no 8 we go through the loop
		                         " for word in wordList "
                                      and check word is not a stopword then add the word in list

@ Synsets for a word
                WordNet is a lexical database for the English language
                Synset instances, which are groupings of synonymous words that express the same concept

                To access the Wrodnet corpus data we need to download 'wordnet' and unzip in corpus folder. So that WordNetCorpusReader can access it.
                To get the Synset of phrase we need to import

                >>> from nltk.corpus import wordnet
                >>> synset= wordnet.synsets('cookbook')[0]
                >>> synset.name()                        # will give you a unique name for the Synset
                >>> synset.definition()                  # should be self-explanatory
                >>> synset.example()                     # which contains a list of phrases that use the word in context

               
@ lemmas and synonyms in WordNet

                lemma :- is the canonical form or morphological form of a word. (how things are put together)
                1 >>> lemmas = syn.lemmas()
                2 >>> for lem in lemmas:
			    3 >>>     print(lem)
			    4 >>>     print(lem.name())
                5 >>> print(lemmas[0].synset() == lemmas[1].synset())

                @ line 1 we retrieve the lemmas of word 'CookBook'. That return two lemmas. And both lemmas are belong to same group is proved in line 5 ie. cookbook.n.01

@ Working with hypernyms
                Synsets are organized in a structure similar to that of an inheritance tree. More abstract terms are known as hypernyms
                Hypernyms provide a way to categorize and group words based on their similarity to each other.
				
				 To get similarity between two words
                  First we need Synsets of two words and then by using wup_similarity (method is short for Wu-Palmer Similarity). which is a scoring method based on how
                  similar the word senses are and where the Synsets occur relative to each other in the hypernym tree

                  >>> from nltk.corpus import wordnet
                  >>> syncook = wordnet.synsets('cookbook')[0]
				  >>> synbook = wordnet.synsets('book')[0]
				  >>> print(syncook.wup_similarity(synbook));
		   output >>>  0.9090909090909091
	
				In this way we found the similarity between two words.

@ Discovering word collocations
				Collocations :- are two or more words that tend to appear frequently together. Like United State, where United is appear in conjunction with other
								words like United Kingdom or United Airlines.

				Discovering collocations in this list of words means that we'll find common phrases that occur frequently throughout the text.
		
		bigrams:- a pair of consecutive written units such as letters, syllables, or words

@ Chapter 2

	@ Stemming words :- Stemming is a technique to remove affixes from a word, ending up with the stem. For example, the stem of cooking is cook, and a good
						stemming algorithm knows that the ing suffix can be removed.
		
		Most commonly used stem algorithm is "Porter stemming algorithm by Martin Porter."

			 	>>> from nltk.stem import PorterStemmer
				>>> stemmer = PorterStemmer()
				>>> print(stemmer.stem('cooking'))
		output  >>> cook                             
				>>> print(stemmer.stem('cookery'))
		output  >>> cookeri                        

				Stemmerl
						|
						|
					  	 --> PorterStemmer
						|
						|
						 --> LancasterStemmer
						|
						|
						 --> RegexStemmer
						|
						|
 						 --> SnowballStemmer

PorterStemmer and LancasterStemmer are both same but slight difference in put like

					Class  				Input                  Output
					PorterStemmer       cookery                cookeri
					LancasterStemmer    cookery                cookery

	RegexStemmer you can define your own regular expression to remove 'ing' from word.
	
		Exp :
			stemmer = RegexpStemmer('ing')
			stemmer.stem('cooking') 		--> 'cook'
			stemmer.stem('ingleside')   	--> 'leside'

	SnowballStemmer -> is the multi language stemmer supporter.

@ Replacing words matching regular expression

		We are going to replace a those words that end with n't/ve/s/re like
					haven't -> have not
					should've -> should not
					We'll -> We will

				To achieve above result we need to create a regular expression set where we kept all of the expression then apply then over the word phase.

@ Lemmatizing words with WordNet
	Lemmatization is very similar to stemming, but is more akin to synonym replacement.A lemma is a root word, as opposed to the root stem.
	Now lets take an example say we had a word 'cooking'. Now the noun representation of 'cooking' is 'cooking' but verb will be cook.

	Example in sentence : "he developed an interest in cooking". Here the cooking is noun
						  "shall I cook dinner tonight?". Here the cook is verb.
						  "while the rice is cooking, add the saffron to the stock". Here cooking is used as present participle.

	Now how we are going to achieve using NLTK
	
			>>> from nltk.stem import WordNetLemmatizer
			>>> lemmatizer =  WordNetLemmatizer()
			>>> print(lemmatizer.lemmatize('studied')) # By default it return the noun phase
	output  >>> studied
			>>> print(lemmatizer.lemmatize('studied', pos='v'))
	output  >>> study

	The WordNetLemmatizer class is a thin wrapper around the wordnet corpus and uses the morphy() function of the WordNetCorpusReader class to find a lemma.
		If no lemma is found then it return the same word or word itself a lemma.
		In this case 'studies' -> not lemma found for noun. but get the verb phase ie. study.

@Replacing words matching regular expressions
		Our first example to replace the contraction word. i.e
				should've --> should have
				can't     --> cannot

		To achieve this goal we need to write a regular expression
			If you check
				won't -> will not -> r'won\'t  --> here 'r' represent regex if the string match won't then replace it with will not
				we'll -> we will  -> r'(\w+)\'ll --> here 'r' represent regex if the string match (\w+) all character end with "'ll". \w+ is a group

			replacement_patterns = [
					(r'won\'t', 'will not'),
					(r'can\'t', 'cannot'),
					(r'i\'m', 'i am'),
					(r'ain\'t', 'is not'),
					(r'(\w+)\'ll', '\g<1> will'),
					(r'(\w+)n\'t', '\g<1> not'),
					(r'(\w+)\'ve', '\g<1> have'),
					(r'(\w+)\'s', '\g<1> is'),
					(r'(\w+)\'re', '\g<1> are'),
					(r'(\w+)\'d', '\g<1> would')
			]

			class RegexpReplacer(object):
					def __init__(self, patterns = replacement_patterns):
							self.pattern = [(re.compile(regex), repl) for (regex, repl) in patterns]

					def replacer(self ,text):
							s = text
							for (expression, repl) in self.pattern:
								s = re.sub(expression, repl, s)
								return s;

					Above we create a init method where we initialize the compiled version of regular expression in self.pattern
					in replacer method we replace that word usng re.sub(<expression>,<expected>, <word>)

Replacing contraction word before tokenize

			import re
			from nltk.tokenize import word_tokenize
			from nltk.corpus import stopwords
			from string import punctuation
			from nltk.stem import WordNetLemmatizer

			# Step we are following
			# 1. replace all the contraction word
			# 2. tokenize all the word
			# 3. remove stop words
			# 4. lematize the words

			str="Alexa'll you please turn all the lights on of my bedroom."

			wordList= word_tokenize(regReplacer.replacer(str))
			print("Printing the word list after contraction =",wordList)
			word = [lemmatizer.lemmatize(word) for word in wordList if word not in stopword_set]
			print("Printing the word list after removing stop words =",word)

@ Replacing negations with antonyms -- Need to complete
			An antonym is a word that has the opposite meaning of another word

@ Creating a wordlist corpus

	To create a custom corpus list we need the WordListCorpusReader class. This class extends CorpusReader which identify the files that need to be
	read while WordListCorpusReader reads the files and tokenizes each line to produce a list of words.

			Example              
			>>>        import nltk.data
			>>> from nltk.corpus.reader import WordListCorpusReader
			>>> wordlist = WordListCorpusReader("C:/nltk_data/corpora/cookbook" ,['wordlist'])
			>>> print(wordlist.words())
	output	>>> ['nltk', 'corpus', 'corpora', 'wordnet']
			>>> print(wordlist.fileids())
	output   >>> ['wordlist'] --> is the file name

@ Creating a part-of-speech tagged word corpus

				CC --> coordinating conjunction
				CD --> cardinal digit
				DT --> determiner
				EX-->existential there (like: “there is” … think of it like “there exists”)
				FW -->foreign word
				IN preposition/subordinating conjunction
				JJ adjective ‘big’
				JJR adjective, comparative ‘bigger’
				JJS adjective, superlative ‘biggest’
				LS list marker 1)
				MD modal could, will
				NN noun, singular ‘desk’
				NNS noun plural ‘desks’
				NNP proper noun, singular ‘Harrison’
				NNPS proper noun, plural ‘Americans’
				PDT predeterminer ‘all the kids’
				POS possessive ending parent‘s
				PRP personal pronoun I, he, she
				PRP$ possessive pronoun my, his, hers
				RB adverb very, silently,
				RBR adverb, comparative better
				RBS adverb, superlative best
				RP particle give up
				TO to go ‘to‘ the store.
				UH interjection errrrrrrrm
				VB verb, base form take
				VBD verb, past tense took
				VBG verb, gerund/present participle taking
				VBN verb, past participle taken
				VBP verb, sing. present, non-3d take
				VBZ verb, 3rd person sing. present takes
				WDT wh-determiner which
				WP wh-pronoun who, what
				WP$ possessive wh-pronoun whose
				WRB wh-abverb where, when

				import nltk.data
				from nltk.corpus.reader import WordListCorpusReader
				from nltk.corpus import names
				from nltk.corpus.reader import TaggedCorpusReader
				from nltk.tokenize import SpaceTokenizer

			1.	reader = TaggedCorpusReader("C:/nltk_data/corpora/treebank/tagged",r'.*\.pos', word_tokenizer=SpaceTokenizer(), tagset='en-brown')
			2.	print(reader.words('wsj_0001.pos'))
			3.	print(reader.tagged_words('wsj_0001.pos'))
			4.	print(reader.tagged_sents('wsj_0001.pos'))
			5.	print(reader.tagged_paras('wsj_0001.pos'))
			6.	print(reader.fileids())
			7.	print("\n")
			8.	print(reader.tagged_words('wsj_0001.pos',tagset='universal'))

TaggedCorpusReader -> The corpus that is been tagged into part-of-speech. It took multiple parameters here we send from which location hit need to pickup
					pos files and what is the tokenizer and tagset. Here tagset indication which tagset the file is belong too. then @ line 2 we read all the words
					in pos file. The read the word with tagset, Sentence with tagset and paragraph with tagset.
					@ line 8 we convert tagset into universal form

Lesk used of word meaning

Classification Problem :-
algo are :- Naive bayes
		Support vector machine
Clusturing Problem:-
	K-Mean and Hierarchical Clusturing

Install
pip install beautifulsoup4
pip install lxml

The FreqDist class is used to encode “frequency distributions”, which count the number of times that each outcome of an experiment occurs.
heapq - nlargest -> is used to sort largest collection
intracluster similaruty